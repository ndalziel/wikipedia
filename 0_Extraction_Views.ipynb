{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction - Wikipedia Views\n",
    "##### a) Imports \n",
    "Let's start by importing some packages that we'll need..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "# File utilities\n",
    "from pathlib import Path\n",
    "from gzip import GzipFile\n",
    "from bz2file import BZ2File\n",
    "import wget\n",
    "\n",
    "# parsing\n",
    "import re\n",
    "from urllib.parse import unquote\n",
    "from typing import Dict, Set, List, Tuple"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll download the pageview files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = \"simple\"\n",
    "rawdatadir = \"\" # \"../rawdata/\"\n",
    "datadir = \"\" # \"../data/\"\n",
    "wikidump = \"https://dumps.wikimedia.org/\" + wiki + \"wiki/latest/\"\n",
    "filenames = {}\n",
    "filenames['article'] = wiki + \"wiki-latest-pages-articles.xml.bz2\"\n",
    "\n",
    "def download_pageviews(*,start_year, end_year, start_month=1, end_month=12):\n",
    "    \"\"\" Download page view data from wiki dumps \"\"\"\n",
    "    views = \"https://dumps.wikimedia.org/other/pageview_complete/monthly/\"\n",
    "\n",
    "    for y in range(start_year, end_year+1):\n",
    "        year_str = str(y)\n",
    "        for m in range(start_month, end_month+1):\n",
    "            month_str = str((m)).zfill(2)\n",
    "            print(\"\\nDownloading views:\", year_str + month_str)\n",
    "            url = views + year_str + \"/\" + year_str + \"-\" + month_str + \"/\"\n",
    "            filename = \"pageviews-\" + year_str + month_str + \"-user.bz2\"\n",
    "            url += filename\n",
    "            try:\n",
    "                Path(rawdatadir + \"/\" + filename).resolve(strict=True)\n",
    "                print (\"View file already downloaded\")\n",
    "            except FileNotFoundError:\n",
    "                wget.download(wikidump+filenames['article'], rawdatadir)\n",
    "                wget.download(url, out=rawdatadir)\n",
    "download_pageviews(start_year=2016,end_year = 2016, start_month=1,end_month=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to pageviews data, a well-formed line of data has 6 elements separated by a space:\n",
    "1. Domain\n",
    "2. Title\n",
    "3. Page ID\n",
    "4. Platform\n",
    "5. Pageviews\n",
    "6. Hourly counts\n",
    "\n",
    "We don't need the hourly counts, but a quick explantion of the format... \"P1R3\" means 1 view in hour 16 (\"P1\") and and 3 views in hour 16 (\"R3\").\n",
    "Unfortunately, the data is messy so we can't always rely on well-formed data. Here's a class which will parse a line of pageview data... Note that we need to double-decode to be able to de-code non-ASCII characters (as in the example above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViewData:\n",
    "    __slots__ = (\"line\",\"log\",\"wikicode\",\"title\", \"pageid\", \"views\", \"desktop\", \"mobile\")\n",
    "    \n",
    "    def __init__(self, line, log = None):\n",
    "        self.line = unquote(unquote(line))\n",
    "        self.log = log\n",
    "        self.title = \"None\"\n",
    "        self.pageid = \"-1\"\n",
    "        self.views = 0\n",
    "        self.desktop = 0\n",
    "        self.mobile = 0 \n",
    "        self.parse()\n",
    "\n",
    "    def __str__(self):\n",
    "            return self.wikicode + \":\" + self.title + \"(\" + self.pageid + \") D\" + str(self.desktop) + \" M\" + str(self.mobile) \n",
    "\n",
    "    def log_error(self,code, output):\n",
    "         if self.log:\n",
    "              with open(self.log,\"a\") as logfile:\n",
    "                   logfile.write(code + \"\\t\" + output)  \n",
    "    \n",
    "    def parse(self): \n",
    "        self.wikicode = self.line[:2]\n",
    "        split = self.line.split(\" \")\n",
    "        if len(split) > 5:\n",
    "            if len(split) == 6:  # expected format \n",
    "                wiki, self.title, self.pageid, platform, views , _ = split\n",
    "            else:\n",
    "                views = split[-2]\n",
    "                platform = split[-3]\n",
    "                self.pageid = split[-4]\n",
    "            try:\n",
    "                self.views = int(views)\n",
    "                self.desktop = self.views if platform == \"desktop\" else 0\n",
    "                self.mobile = self.views if platform != \"desktop\" else 0\n",
    "            except:\n",
    "                self.log_error(\"L\",self.line) \n",
    "        else:\n",
    "             self.log_error(\"S\",self.line)\n",
    "                "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll validate that this parses some test cases as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data = \"en.wikipedia 2010_world_cup_matches 168079 desktop 2 B2\"\n",
    "missing_page_id = \"en.wikipedia Algorithms null desktop 8 A2B1G1K1R1Z1[1\"\n",
    "html_encoded = \"en.wikipedia %25C3%2596zel:Ara 41416740 desktop 4 P1R3\"\n",
    "non_article = \"en.wikipedia Category:Belgian_wine 24687607 mobile-web 6 D2T2]1_1\"\n",
    "space_in_title = \"en.wikipedia Deep belief network 41416740 desktop 1 C1\"\n",
    "missing_data = \"en.wikipedia Concept_map\"\n",
    "\n",
    "print(ViewData(good_data))\n",
    "print(ViewData(missing_page_id))\n",
    "print(ViewData(html_encoded))\n",
    "print(ViewData(non_article))\n",
    "print(ViewData(space_in_title))\n",
    "print(ViewData(missing_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll process the views data file. We'll only keep the data if it's a valid article (based on the pageid). This will throw away pages like the category page in the example above. We'll also check that the number of views is greater than zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pageviews(*, log = None, writemode = \"a\", pagefile = \"pageviews.tsv.bz2\", projectfile = \"projectviews.tsv\", \n",
    "                      wikis, start_year:int, end_year:int, start_month:int, end_month:int, rawdatadir:str, datadir:str):\n",
    "    with BZ2File( datadir + pagefile, writemode) as outfile:\n",
    "        with open(datadir + projectfile, writemode) as sumfile:\n",
    "            for year in range(start_year, end_year+1):    \n",
    "                for month in range(start_month, end_month+1):\n",
    "                    infile_name = rawdatadir + \"pageviews-\" + str(year) + str((month)).zfill(2) + \"-user.bz2\"\n",
    "                    with BZ2File(infile_name) as infile:\n",
    "                        print(\"Processing views:\", str(year) + str((month)).zfill(2))\n",
    "                        if writemode == \"w\":\n",
    "                            outfile.write(\"wiki\\tyear\\tmonth\\tpageid\\tdesktop\\tmobile\\n\".encode())\n",
    "                            sumfile.write(\"wiki\\tyear\\tmonth\\tviews\\n\")\n",
    "                        for wiki in wikis:\n",
    "                            wikiviews = 0\n",
    "                            with open(rawdatadir + wiki.split(\".\",1)[0] + 'wiki-title-pageid.pickle', 'rb') as handle:\n",
    "                                data = pickle.load(handle)\n",
    "                            pages = set(data.values())\n",
    "                            infile.seek(0,0)  \n",
    "                            for line in infile:\n",
    "                                if line.decode().startswith(wiki):\n",
    "                                    viewdata = ViewData(line,log) \n",
    "                                    if viewdata.views > 0:\n",
    "                                        wikiviews += viewdata.views\n",
    "                                        if viewdata.pageid:\n",
    "                                            if viewdata.pageid in pages:\n",
    "                                                key = viewdata.wikicode + \"\\t\" + str(year) + \"\\t\" + str(month) \n",
    "                                                views = viewdata.pageid + \"\\t\" + str(viewdata.desktop) + \"\\t\" + str(viewdata.mobile)\n",
    "                                                outfile.write((key + \"\\t\" + views + \"\\n\").encode())\n",
    "                            sumfile.write(wiki + \"\\t\" + str(year) + \"\\t\" + str(month) + \"\\t\" + str(wikiviews) + \"\\n\")                            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_pageviews (writemode = \"w\",\n",
    "                    wikis = (\"en.wikipedia\",\"fr.wikipedia\",\"simple.wikipedia\"),\n",
    "                    start_year = 2016,\n",
    "                    end_year = 2016,\n",
    "                    start_month = 1,\n",
    "                    end_month = 1,\n",
    "                    rawdatadir = \"../rawdata/\",\n",
    "                    datadir = \"../data/\")\n",
    "\n",
    "views = pd.read_table(\n",
    "        \"../data/pageviews.tsv.bz2\", \n",
    "        dtype = {\"wiki\":\"str\",\"year\":\"int16\", \"month\":\"int8\",\"pageid\":\"int32\",\n",
    "                 \"desktop\":\"int32\", \"mobile\":\"int32\"},\n",
    "        keep_default_na=False, \n",
    "        na_values=['_'],\n",
    "        quoting = 3,\n",
    "        iterator = True).get_chunk(100)\n",
    "print(views.head())\n",
    "pd.read_table(\"../data/projectviews.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
