{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction - Wikipedia Pages\n",
    "##### a) Imports \n",
    "Let's start by importing some packages that we'll need..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "# File utilities\n",
    "from pathlib import Path\n",
    "from gzip import GzipFile\n",
    "from bz2file import BZ2File\n",
    "import wget\n",
    "\n",
    "# parsing\n",
    "from lxml import etree\n",
    "import re\n",
    "import wikitextparser as wtp\n",
    "from urllib.parse import unquote\n",
    "from typing import Dict, Set, List, Tuple"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Download page content\n",
    " Note that there are many different [wikipedia sites](https://meta.wikimedia.org/wiki/List_of_Wikipedias\"). The subdomain for the wikipedia site is usually the 2-letter [international language code](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) for the language in which the wikipedia is written. There are exceptions, such as the simple English wikipedia - [simple.wikipedia.org](https://simple.wikipedia.org/wiki/Main_Page). We'll begin by downloading the page content from the Wikimedia site...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = \"en\"\n",
    "rawdatadir = \"\" # \"../rawdata/\"\n",
    "datadir = \"\" # \"../data/\"\n",
    "wikidump = \"https://dumps.wikimedia.org/\" + wiki + \"wiki/latest/\"\n",
    "filenames = {}\n",
    "filenames['article'] = wiki + \"wiki-latest-pages-articles.xml.bz2\"\n",
    "try:\n",
    "    Path(rawdatadir + \"/\" + filenames['article']).resolve(strict=True)\n",
    "    print (\"Articles file already downloaded\")\n",
    "except FileNotFoundError:\n",
    "    wget.download(wikidump+filenames['article'], rawdatadir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) Parse page XML file for content\n",
    "The pages file is an XML file. We'll use the etree package to parse the xml.  We'll need a function that can extract the relevant data elements from the XML..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageData:\n",
    "    __slots__ = (\"page\",\"title\",\"ns\",'pageid','redirect','rd_title','wikitext','pageweight')\n",
    "    def __init__(self, page):\n",
    "        self.page = page\n",
    "        self.parse()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.title + \"(\" + self.pageid + \"): \" + self.wikitext\n",
    "\n",
    "    def parse(self):\n",
    "        self.redirect = False\n",
    "        for child in self.page:\n",
    "            name = etree.QName(child).localname\n",
    "            if name == \"title\":\n",
    "                self.title = child.text\n",
    "            elif name == \"ns\":\n",
    "                self.ns = child.text\n",
    "            elif name == \"id\":\n",
    "                self.pageid = child.text\n",
    "            elif name == \"redirect\":\n",
    "                self.redirect = True\n",
    "                self.rd_title = child.attrib['title']\n",
    "            elif name == \"revision\":\n",
    "                for grandchild in child:\n",
    "                    name = etree.QName(grandchild).localname\n",
    "                    if name == \"text\":\n",
    "                        if grandchild.text:\n",
    "                            self.wikitext = grandchild.text.replace('\\n', ' ').replace('\\t', ' ')  # replace tabs\n",
    "                        self.pageweight = grandchild.attrib['bytes']\n",
    "                        try:\n",
    "                            int(self.pageweight)\n",
    "                        except:\n",
    "                            self.pageweight = 0\n",
    "            else:\n",
    "                pass\n",
    "            if not self.redirect:\n",
    "                self.rd_title = self.title"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simplfied sample of the page content for testing. Let's run the function on the sample of XML data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April(1):  April is the fourth [[month]] of the [[year]] \n"
     ]
    }
   ],
   "source": [
    "testpage=\" \\\n",
    "<page> \\\n",
    "  <title>April</title> \\\n",
    "  <ns>0</ns> \\\n",
    "  <id>1</id> \\\n",
    "  <revision> \\\n",
    "    <id>8446859</id> \\\n",
    "    <text bytes = \\\"22188\\\"> April is the fourth [[month]] of the [[year]] </text> \\\n",
    "    <sha1>iyw2lle520lh9mgpxgg1y0age5yr5b5</sha1> \\\n",
    "  </revision> \\\n",
    "</page>\"\n",
    "\n",
    "page = etree.fromstring(testpage)\n",
    "print(PageData(page))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll parse the page data. Note that we won't be loading all the data into memory. The etree package allows us to iteratively traverse the XML tree with teh \"iterparse\" method. In addition to a content file, we'll also create a pagemaster file. This will include a flag for whether the page redirects to another page and the title of the redirect page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pages(*,wiki, rawdatadir = \"\", datadir = \"\"):\n",
    "    article_file = wiki + \"wiki-latest-pages-articles.xml.bz2\"\n",
    "    title_redirect_dict = {}\n",
    "    title_pageid_dict = {}\n",
    "\n",
    "    with BZ2File(rawdatadir + article_file) as infile:\n",
    "        with BZ2File(datadir + wiki + \"wiki-pagemaster.tsv.bz2\", \"w\") as pagemasterfile:\n",
    "            pagemasterfile.write((\"pageid\\ttitle\\trdflag\\tredirect\\tpageweight\\n\").encode())\n",
    "            with BZ2File(datadir + wiki + \"wiki-pages.tsv.bz2\", \"w\") as pagefile:\n",
    "                pagefile.write((\"pageid\\ttitle\\twikitext\\n\").encode())\n",
    "                wiki_ns = \"{http://www.mediawiki.org/xml/export-0.10/}\"\n",
    "\n",
    "                for _, page in etree.iterparse(infile, tag=wiki_ns + \"page\"):\n",
    "\n",
    "                    article = PageData(page)\n",
    "                    if article.ns == \"0\": #article \n",
    "                        title_redirect_dict[article.title.lower()] = article.rd_title\n",
    "                        title_pageid_dict[article.title] = article.pageid   \n",
    "                        output = article.pageid + \"\\t\" + article.title + \"\\t\" + str(int(article.redirect)) + \"\\t\" + article.rd_title + \"\\t\" \n",
    "                        output += article.pageweight + \"\\n\"\n",
    "                        pagemasterfile.write(output.encode())\n",
    "                        if not article.redirect:\n",
    "                            pagefile.write((article.pageid + \"\\t\" + article.title + \"\\t\" + article.wikitext + \"\\n\").encode())   \n",
    "                    page.clear(keep_tail=True)\n",
    "                    \n",
    "                with open(rawdatadir + wiki + 'wiki-title-redirect.pickle', 'wb') as handle:\n",
    "                    pickle.dump(title_redirect_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                with open(rawdatadir + wiki + 'wiki-title-pageid.pickle', 'wb') as handle:\n",
    "                    pickle.dump(title_pageid_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's process and load the data. Note that we're specifying the numeric data types to limit memory usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/nigeldalziel/Documents/MCIT/5_bd/wikipedia/0_Extraction_Pages.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nigeldalziel/Documents/MCIT/5_bd/wikipedia/0_Extraction_Pages.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nigeldalziel/Documents/MCIT/5_bd/wikipedia/0_Extraction_Pages.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     Path(pagemasterfile)\u001b[39m.\u001b[39;49mresolve(strict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nigeldalziel/Documents/MCIT/5_bd/wikipedia/0_Extraction_Pages.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mPage master file already exists\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/pathlib.py:1215\u001b[0m, in \u001b[0;36mPath.resolve\u001b[0;34m(self, strict)\u001b[0m\n\u001b[1;32m   1210\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m \u001b[39mMake the path absolute, resolving all symlinks on the way and also\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m \u001b[39mnormalizing it (for example turning slashes into backslashes under\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[39mWindows).\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1215\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flavour\u001b[39m.\u001b[39;49mresolve(\u001b[39mself\u001b[39;49m, strict\u001b[39m=\u001b[39;49mstrict)\n\u001b[1;32m   1216\u001b[0m \u001b[39mif\u001b[39;00m s \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1217\u001b[0m     \u001b[39m# No symlink resolution => for consistency, raise an error if\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m     \u001b[39m# the path doesn't exist or is forbidden\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/pathlib.py:373\u001b[0m, in \u001b[0;36m_PosixFlavour.resolve\u001b[0;34m(self, path, strict)\u001b[0m\n\u001b[1;32m    372\u001b[0m base \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m path\u001b[39m.\u001b[39mis_absolute() \u001b[39melse\u001b[39;00m os\u001b[39m.\u001b[39mgetcwd()\n\u001b[0;32m--> 373\u001b[0m \u001b[39mreturn\u001b[39;00m _resolve(base, \u001b[39mstr\u001b[39;49m(path)) \u001b[39mor\u001b[39;00m sep\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/pathlib.py:357\u001b[0m, in \u001b[0;36m_PosixFlavour.resolve.<locals>._resolve\u001b[0;34m(path, rest)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 357\u001b[0m     target \u001b[39m=\u001b[39m accessor\u001b[39m.\u001b[39;49mreadlink(newpath)\n\u001b[1;32m    358\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/pathlib.py:462\u001b[0m, in \u001b[0;36m_NormalAccessor.readlink\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreadlink\u001b[39m(\u001b[39mself\u001b[39m, path):\n\u001b[0;32m--> 462\u001b[0m     \u001b[39mreturn\u001b[39;00m os\u001b[39m.\u001b[39;49mreadlink(path)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/nigeldalziel/Documents/MCIT/5_bd/wikipedia/enwiki-pagemaster.tsv.bz2'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/nigeldalziel/Documents/MCIT/5_bd/wikipedia/0_Extraction_Pages.ipynb Cell 12\u001b[0m in \u001b[0;36mprocess_pages\u001b[0;34m(wiki, rawdatadir, datadir)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nigeldalziel/Documents/MCIT/5_bd/wikipedia/0_Extraction_Pages.ipynb#X15sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m article\u001b[39m.\u001b[39mredirect:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nigeldalziel/Documents/MCIT/5_bd/wikipedia/0_Extraction_Pages.ipynb#X15sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         pagefile\u001b[39m.\u001b[39;49mwrite((article\u001b[39m.\u001b[39;49mpageid \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m article\u001b[39m.\u001b[39;49mtitle \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m article\u001b[39m.\u001b[39;49mwikitext \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mencode())   \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nigeldalziel/Documents/MCIT/5_bd/wikipedia/0_Extraction_Pages.ipynb#X15sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m page\u001b[39m.\u001b[39mclear(keep_tail\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/bz2file.py:389\u001b[0m, in \u001b[0;36mBZ2File.write\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    388\u001b[0m compressed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compressor\u001b[39m.\u001b[39mcompress(data)\n\u001b[0;32m--> 389\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mwrite(compressed)\n\u001b[1;32m    390\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pos \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/nigeldalziel/Documents/MCIT/5_bd/wikipedia/0_Extraction_Pages.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nigeldalziel/Documents/MCIT/5_bd/wikipedia/0_Extraction_Pages.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mPage master file already exists\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nigeldalziel/Documents/MCIT/5_bd/wikipedia/0_Extraction_Pages.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nigeldalziel/Documents/MCIT/5_bd/wikipedia/0_Extraction_Pages.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     process_pages(wiki\u001b[39m=\u001b[39;49mwiki,rawdatadir \u001b[39m=\u001b[39;49m rawdatadir ,datadir \u001b[39m=\u001b[39;49m datadir)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nigeldalziel/Documents/MCIT/5_bd/wikipedia/0_Extraction_Pages.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m pagemasterfile \u001b[39m=\u001b[39m datadir \u001b[39m+\u001b[39m  wiki \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwiki-pagemaster.tsv.bz2\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nigeldalziel/Documents/MCIT/5_bd/wikipedia/0_Extraction_Pages.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m pagemaster \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_table(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nigeldalziel/Documents/MCIT/5_bd/wikipedia/0_Extraction_Pages.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     pagemasterfile, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nigeldalziel/Documents/MCIT/5_bd/wikipedia/0_Extraction_Pages.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     dtype \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpageid\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mint32\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mrdflag\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mint8\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpageweight\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mint32\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nigeldalziel/Documents/MCIT/5_bd/wikipedia/0_Extraction_Pages.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     quoting \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nigeldalziel/Documents/MCIT/5_bd/wikipedia/0_Extraction_Pages.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     iterator \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mget_chunk(\u001b[39m100\u001b[39m)\n",
      "\u001b[1;32m/Users/nigeldalziel/Documents/MCIT/5_bd/wikipedia/0_Extraction_Pages.ipynb Cell 12\u001b[0m in \u001b[0;36mprocess_pages\u001b[0;34m(wiki, rawdatadir, datadir)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nigeldalziel/Documents/MCIT/5_bd/wikipedia/0_Extraction_Pages.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     pickle\u001b[39m.\u001b[39mdump(title_redirect_dict, handle, protocol\u001b[39m=\u001b[39mpickle\u001b[39m.\u001b[39mHIGHEST_PROTOCOL)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nigeldalziel/Documents/MCIT/5_bd/wikipedia/0_Extraction_Pages.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(rawdatadir \u001b[39m+\u001b[39m wiki \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mwiki-title-pageid.pickle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m handle:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nigeldalziel/Documents/MCIT/5_bd/wikipedia/0_Extraction_Pages.ipynb#X15sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     pickle\u001b[39m.\u001b[39mdump(title_pageid_dict, handle, protocol\u001b[39m=\u001b[39mpickle\u001b[39m.\u001b[39mHIGHEST_PROTOCOL)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/bz2file.py:130\u001b[0m, in \u001b[0;36mBZ2File.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode \u001b[39m==\u001b[39m _MODE_WRITE:\n\u001b[0;32m--> 130\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mwrite(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compressor\u001b[39m.\u001b[39;49mflush())\n\u001b[1;32m    131\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compressor \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "pagemasterfile = datadir  +  wiki + \"wiki-pagemaster.tsv.bz2\"\n",
    "try:\n",
    "    Path(pagemasterfile).resolve(strict=True)\n",
    "    print (\"Page master file already exists\")\n",
    "except FileNotFoundError:\n",
    "    process_pages(wiki=wiki,rawdatadir = rawdatadir ,datadir = datadir)\n",
    "\n",
    "pagemasterfile = datadir +  wiki + \"wiki-pagemaster.tsv.bz2\"\n",
    "pagemaster = pd.read_table(\n",
    "    pagemasterfile, \n",
    "    dtype = {\"pageid\":\"int32\",\"rdflag\":\"int8\", \"pageweight\":\"int32\",\n",
    "    \"sections\":\"int16\", \"wikilinks\":\"int16\",\"extlinks\":\"int16\"},\n",
    "    keep_default_na=False, \n",
    "    na_values=['_'],\n",
    "    quoting = 3,\n",
    "    iterator = True).get_chunk(100)\n",
    "pagemaster.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also take a look at the page file. We'll use a parsing library - wikitextparser - to convert the wikitext into plaintext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagefile = datadir + wiki + \"wiki-pages.tsv.bz2\"\n",
    "pages = pd.read_table(\n",
    "        pagefile, \n",
    "        dtype = {\"pageid\":\"int32\"},\n",
    "        keep_default_na=False, \n",
    "        na_values=['_'],\n",
    "        quoting = 3,\n",
    "        iterator = True).get_chunk(100)\n",
    "\n",
    "def wikitext_to_plaintext(wikitext):\n",
    "    return wtp.parse(wikitext).plain_text()\n",
    "\n",
    "pages[\"plaintext\"]=pages[\"wikitext\"].map(wikitext_to_plaintext)\n",
    "\n",
    "pages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
