{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction - Wikipedia Pages\n",
    "##### a) Imports \n",
    "Let's start by importing some packages that we'll need..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "# File utilities\n",
    "from pathlib import Path\n",
    "from gzip import GzipFile\n",
    "from bz2file import BZ2File\n",
    "import wget\n",
    "\n",
    "# parsing\n",
    "from lxml import etree\n",
    "import re\n",
    "import wikitextparser as wtp\n",
    "from urllib.parse import unquote\n",
    "from typing import Dict, Set, List, Tuple"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Download page content\n",
    " Note that there are many different [wikipedia sites](https://meta.wikimedia.org/wiki/List_of_Wikipedias\"). The subdomain for the wikipedia site is usually the 2-letter [international language code](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) for the language in which the wikipedia is written. There are exceptions, such as the simple English wikipedia - [simple.wikipedia.org](https://simple.wikipedia.org/wiki/Main_Page). We'll begin by downloading the page content from the Wikimedia site...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = \"simple\"\n",
    "rawdatadir = \"\" # \"../rawdata/\"\n",
    "datadir = \"\" # \"../data/\"\n",
    "wikidump = \"https://dumps.wikimedia.org/\" + wiki + \"wiki/latest/\"\n",
    "filenames = {}\n",
    "filenames['article'] = wiki + \"wiki-latest-pages-articles.xml.bz2\"\n",
    "try:\n",
    "    Path(rawdatadir + \"/\" + filenames['article']).resolve(strict=True)\n",
    "    print (\"Articles file already downloaded\")\n",
    "except FileNotFoundError:\n",
    "    wget.download(wikidump+filenames['article'], rawdatadir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) Parse page XML file for content\n",
    "The pages file is an XML file. We'll use the etree package to parse the xml.  We'll need a function that can extract the relevant data elements from the XML..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageData:\n",
    "    __slots__ = (\"page\",\"title\",\"ns\",'pageid','redirect','rd_title','wikitext','pageweight')\n",
    "    def __init__(self, page):\n",
    "        self.page = page\n",
    "        self.parse()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.title + \"(\" + self.pageid + \"): \" + self.wikitext\n",
    "\n",
    "    def parse(self):\n",
    "        self.redirect = False\n",
    "        for child in self.page:\n",
    "            name = etree.QName(child).localname\n",
    "            if name == \"title\":\n",
    "                self.title = child.text\n",
    "            elif name == \"ns\":\n",
    "                self.ns = child.text\n",
    "            elif name == \"id\":\n",
    "                self.pageid = child.text\n",
    "            elif name == \"redirect\":\n",
    "                self.redirect = True\n",
    "                self.rd_title = child.attrib['title']\n",
    "            elif name == \"revision\":\n",
    "                for grandchild in child:\n",
    "                    name = etree.QName(grandchild).localname\n",
    "                    if name == \"text\":\n",
    "                        if grandchild.text:\n",
    "                            self.wikitext = grandchild.text.replace('\\n', ' ').replace('\\t', ' ')  # replace tabs\n",
    "                        self.pageweight = grandchild.attrib['bytes']\n",
    "                        try:\n",
    "                            int(self.pageweight)\n",
    "                        except:\n",
    "                            self.pageweight = 0\n",
    "            else:\n",
    "                pass\n",
    "            if not self.redirect:\n",
    "                self.rd_title = self.title"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simplfied sample of the page content for testing. Let's run the function on the sample of XML data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April(1):  April is the fourth [[month]] of the [[year]] \n"
     ]
    }
   ],
   "source": [
    "testpage=\" \\\n",
    "<page> \\\n",
    "  <title>April</title> \\\n",
    "  <ns>0</ns> \\\n",
    "  <id>1</id> \\\n",
    "  <revision> \\\n",
    "    <id>8446859</id> \\\n",
    "    <text bytes = \\\"22188\\\"> April is the fourth [[month]] of the [[year]] </text> \\\n",
    "    <sha1>iyw2lle520lh9mgpxgg1y0age5yr5b5</sha1> \\\n",
    "  </revision> \\\n",
    "</page>\"\n",
    "\n",
    "page = etree.fromstring(testpage)\n",
    "print(PageData(page))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll parse the page data. Note that we won't be loading all the data into memory. The etree package allows us to iteratively traverse the XML tree with teh \"iterparse\" method. In addition to a content file, we'll also create a pagemaster file. This will include a flag for whether the page redirects to another page and the title of the redirect page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pages(*,wiki, rawdatadir = \"\", datadir = \"\"):\n",
    "    article_file = wiki + \"wiki-latest-pages-articles.xml.bz2\"\n",
    "    title_redirect_dict = {}\n",
    "    title_pageid_dict = {}\n",
    "\n",
    "    with BZ2File(rawdatadir + article_file) as infile:\n",
    "        with BZ2File(datadir + wiki + \"wiki-pagemaster.tsv.bz2\", \"w\") as pagemasterfile:\n",
    "            pagemasterfile.write((\"pageid\\ttitle\\trdflag\\tredirect\\tpageweight\\n\").encode())\n",
    "            with BZ2File(datadir + wiki + \"wiki-pages.tsv.bz2\", \"w\") as pagefile:\n",
    "                pagefile.write((\"pageid\\ttitle\\twikitext\\n\").encode())\n",
    "                wiki_ns = \"{http://www.mediawiki.org/xml/export-0.10/}\"\n",
    "\n",
    "                for _, page in etree.iterparse(infile, tag=wiki_ns + \"page\"):\n",
    "\n",
    "                    article = PageData(page)\n",
    "                    if article.ns == \"0\": #article \n",
    "                        title_redirect_dict[article.title.lower()] = article.rd_title\n",
    "                        title_pageid_dict[article.title] = article.pageid   \n",
    "                        output = article.pageid + \"\\t\" + article.title + \"\\t\" + str(int(article.redirect)) + \"\\t\" + article.rd_title + \"\\t\" \n",
    "                        output += article.pageweight + \"\\n\"\n",
    "                        pagemasterfile.write(output.encode())\n",
    "                        if not article.redirect:\n",
    "                            pagefile.write((article.pageid + \"\\t\" + article.title + \"\\t\" + article.wikitext + \"\\n\").encode())   \n",
    "                    page.clear(keep_tail=True)\n",
    "                    \n",
    "                with open(rawdatadir + wiki + 'wiki-title-redirect.pickle', 'wb') as handle:\n",
    "                    pickle.dump(title_redirect_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                with open(rawdatadir + wiki + 'wiki-title-pageid.pickle', 'wb') as handle:\n",
    "                    pickle.dump(title_pageid_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's process and load the data. Note that we're specifying the numeric data types to limit memory usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pageid</th>\n",
       "      <th>title</th>\n",
       "      <th>rdflag</th>\n",
       "      <th>redirect</th>\n",
       "      <th>pageweight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>April</td>\n",
       "      <td>0</td>\n",
       "      <td>April</td>\n",
       "      <td>22188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>August</td>\n",
       "      <td>0</td>\n",
       "      <td>August</td>\n",
       "      <td>13326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>Art</td>\n",
       "      <td>0</td>\n",
       "      <td>Art</td>\n",
       "      <td>7655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>3182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>Air</td>\n",
       "      <td>0</td>\n",
       "      <td>Air</td>\n",
       "      <td>4328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pageid   title  rdflag redirect  pageweight\n",
       "0       1   April       0    April       22188\n",
       "1       2  August       0   August       13326\n",
       "2       6     Art       0      Art        7655\n",
       "3       8       A       0        A        3182\n",
       "4       9     Air       0      Air        4328"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagemasterfile = datadir  +  wiki + \"wiki-pagemaster.tsv.bz2\"\n",
    "try:\n",
    "    Path(pagemasterfile).resolve(strict=True)\n",
    "    print (\"Page master file already exists\")\n",
    "except FileNotFoundError:\n",
    "    process_pages(wiki=wiki,rawdatadir = rawdatadir ,datadir = datadir)\n",
    "\n",
    "pagemasterfile = datadir +  wiki + \"wiki-pagemaster.tsv.bz2\"\n",
    "pagemaster = pd.read_table(\n",
    "    pagemasterfile, \n",
    "    dtype = {\"pageid\":\"int32\",\"rdflag\":\"int8\", \"pageweight\":\"int32\",\n",
    "    \"sections\":\"int16\", \"wikilinks\":\"int16\",\"extlinks\":\"int16\"},\n",
    "    keep_default_na=False, \n",
    "    na_values=['_'],\n",
    "    quoting = 3,\n",
    "    iterator = True).get_chunk(100)\n",
    "pagemaster.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also take a look at the page file. We'll use a parsing library - wikitextparser - to convert the wikitext into plaintext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pageid</th>\n",
       "      <th>title</th>\n",
       "      <th>wikitext</th>\n",
       "      <th>plaintext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>April</td>\n",
       "      <td>{{monththisyear|4}} '''April''' is the fourth ...</td>\n",
       "      <td>April is the fourth month of the year in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>August</td>\n",
       "      <td>{{monththisyear|8}} '''August''' (Aug.) is the...</td>\n",
       "      <td>August (Aug.) is the eighth month of the year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>Art</td>\n",
       "      <td>[[File:Pierre-Auguste_Renoir,_Le_Moulin_de_la_...</td>\n",
       "      <td>Art is a creative activity and technical ski...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>A</td>\n",
       "      <td>{{about| the first [[letter]] in the [[alphabe...</td>\n",
       "      <td>thumb|Writing \"A\" in cursive font.  A or a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>Air</td>\n",
       "      <td>[[Image:Kawasaki-Electric Fan.jpg|thumb|A [[wi...</td>\n",
       "      <td>Air is the Earth's atmosphere. Air is a mixt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pageid   title                                           wikitext  \\\n",
       "0       1   April  {{monththisyear|4}} '''April''' is the fourth ...   \n",
       "1       2  August  {{monththisyear|8}} '''August''' (Aug.) is the...   \n",
       "2       6     Art  [[File:Pierre-Auguste_Renoir,_Le_Moulin_de_la_...   \n",
       "3       8       A  {{about| the first [[letter]] in the [[alphabe...   \n",
       "4       9     Air  [[Image:Kawasaki-Electric Fan.jpg|thumb|A [[wi...   \n",
       "\n",
       "                                           plaintext  \n",
       "0   April is the fourth month of the year in the ...  \n",
       "1   August (Aug.) is the eighth month of the year...  \n",
       "2    Art is a creative activity and technical ski...  \n",
       "3     thumb|Writing \"A\" in cursive font.  A or a ...  \n",
       "4    Air is the Earth's atmosphere. Air is a mixt...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagefile = datadir + wiki + \"wiki-pages.tsv.bz2\"\n",
    "pages = pd.read_table(\n",
    "        pagefile, \n",
    "        dtype = {\"pageid\":\"int32\"},\n",
    "        keep_default_na=False, \n",
    "        na_values=['_'],\n",
    "        quoting = 3,\n",
    "        iterator = True).get_chunk(100)\n",
    "\n",
    "def wikitext_to_plaintext(wikitext):\n",
    "    return wtp.parse(wikitext).plain_text()\n",
    "\n",
    "pages[\"plaintext\"]=pages[\"wikitext\"].map(wikitext_to_plaintext)\n",
    "\n",
    "pages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
